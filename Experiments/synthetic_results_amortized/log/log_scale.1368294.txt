2023-12-25 16:17:43.731055: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-25 16:17:43.832743: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2023-12-25 16:17:43.832785: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2023-12-25 16:17:44.850752: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2023-12-25 16:17:44.850815: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2023-12-25 16:17:44.850821: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-12-25 16:17:47.057864: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2023-12-25 16:17:47.057909: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)
2023-12-25 16:17:47.057932: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cn03): /proc/driver/nvidia/version does not exist
2023-12-25 16:17:47.058127: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO:root:Performing 2 pilot runs with the SimpleFroehlichModel model...
INFO:root:Shape of parameter batch after 2 pilot simulations: (batch_size = 2, 6)
INFO:root:Shape of simulation batch after 2 pilot simulations: (batch_size = 2, 180, 1)
INFO:root:No optional prior non-batchable context provided.
INFO:root:No optional prior batchable context provided.
INFO:root:No optional simulation non-batchable context provided.
INFO:root:No optional simulation batchable context provided.
INFO:root:Loaded loss history from networks/amortizer-simple-fro-sequence-summary-Bi-LSTM-6layers-2coupling-spline-500epochs/history_416.pkl.
INFO:root:Networks loaded from networks/amortizer-simple-fro-sequence-summary-Bi-LSTM-6layers-2coupling-spline-500epochs/ckpt-416
INFO:root:Performing a consistency check with provided components...
INFO:root:Done.
using 2 layers of MultiConv1D, a bidirectional LSTM with 256 units and a dense layer with output dimension 12 as summary network
using a 6-layer cINN as inference network with 2 layers of design spline
prior mean: [-3. -3.  5.  0.  0. -1.]
prior covariance diagonal: [ 5.  5. 11.  2.  6.  2.]
Using the model SimpleFroehlichModel
Model: "amortized_posterior"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 invertible_network (Inverti  multiple                 450420    
 bleNetwork)                                                     
                                                                 
 sequence_network (SequenceN  multiple                 605484    
 etwork)                                                         
                                                                 
=================================================================
Total params: 1,055,904
Trainable params: 1,055,832
Non-trainable params: 72
_________________________________________________________________
None
250 samples
100 individuals
['pop-$\\delta$', 'pop-$\\gamma$', 'pop-$k m_0$-scale', 'pop-$t_0$', 'pop-offset', 'pop-$\\sigma$', 'var-$\\delta$', 'var-$\\gamma$', 'var-$k m_0$-scale', 'var-$t_0$', 'var-offset', 'var-$\\sigma$']
## Optimization Result 

* number of starts: 100 
* best value: -1432.6723120402837, id=batch_84_0
* worst value: -1430.631627012025, id=batch_11_0
* number of non-finite values: 0

* execution time summary:
	* Mean execution time: 6.783s
	* Maximum execution time: 9.187s,	id=batch_22_0
	* Minimum execution time: 4.519s,	id=batch_64_0
* summary of optimizer messages:

  |   Count | Message                                         |
  |--------:|:------------------------------------------------|
  |     100 | CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH |

* best value found (approximately) 49 time(s)
* number of plateaus found: 3

A summary of the best run:

### Optimizer Result

* optimizer used: <ScipyOptimizer method=L-BFGS-B options={'disp': False, 'maxfun': 1000}>
* message: CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH 
* number of evaluations: 140
* time taken to optimize: 6.071s
* startpoint: [ -8.81041449   2.99323913  10.87565848  -3.8982184   -3.66375137
  -4.99882125  -7.87698229  -8.17349218  -9.2335608  -10.0108026
   0.39483044]
* endpoint: [-6.62925125 -0.66002486  6.22189555 -0.23554254  2.07866038 -3.47081052
 -1.64653868 -0.05532005  5.79701647  2.0978275   6.90775528]
* final objective value: -1432.6723120402837
* final gradient value: [-7.32097760e-03 -4.73082764e-02  4.90376806e-02  6.66636879e-02
  6.53399184e-02 -4.23849542e-02  5.16001819e-03 -4.83851181e-04
 -5.28007149e-03 -3.53716132e-02 -2.22888502e+01]

