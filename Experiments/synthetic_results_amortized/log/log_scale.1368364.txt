2023-12-26 16:06:25.603905: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-26 16:06:25.915786: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2023-12-26 16:06:25.915826: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2023-12-26 16:06:27.102407: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2023-12-26 16:06:27.102607: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2023-12-26 16:06:27.102613: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-12-26 16:06:30.178584: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2023-12-26 16:06:30.178814: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)
2023-12-26 16:06:30.178832: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cn05): /proc/driver/nvidia/version does not exist
2023-12-26 16:06:30.179326: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO:root:Performing 2 pilot runs with the SDEFroehlichModel model...
INFO:root:Shape of parameter batch after 2 pilot simulations: (batch_size = 2, 8)
INFO:root:Shape of simulation batch after 2 pilot simulations: (batch_size = 2, 180, 1)
INFO:root:No optional prior non-batchable context provided.
INFO:root:No optional prior batchable context provided.
INFO:root:No optional simulation non-batchable context provided.
INFO:root:No optional simulation batchable context provided.
INFO:root:Loaded loss history from networks/amortizer-sde-fro-sequence-summary-LSTM-7layers-3coupling-spline-500epochs/history_417.pkl.
INFO:root:Networks loaded from networks/amortizer-sde-fro-sequence-summary-LSTM-7layers-3coupling-spline-500epochs/ckpt-417
INFO:root:Performing a consistency check with provided components...
INFO:root:Done.
using 2 layers of MultiConv1D, a  LSTM with 256 units and a dense layer with output dimension 16 as summary network
using a 7-layer cINN as inference network with 3 layers of design spline
prior mean: [-3. -3. -1.  5.  0.  0.  0. -1.]
prior covariance diagonal: [5. 5. 5. 5. 5. 2. 5. 2.]
Using the model SDEFroehlichModel
Model: "amortized_posterior"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 invertible_network (Inverti  multiple                 854168    
 bleNetwork)                                                     
                                                                 
 sequence_network (SequenceN  multiple                 307504    
 etwork)                                                         
                                                                 
=================================================================
Total params: 1,161,672
Trainable params: 1,161,560
Non-trainable params: 112
_________________________________________________________________
None
100 samples
10000 individuals
['pop-$\\delta$', 'pop-$\\gamma$', 'pop-$k$', 'pop-$m_0$', 'pop-scale', 'pop-$t_0$', 'pop-offset', 'pop-$\\sigma$', 'var-$\\delta$', 'var-$\\gamma$', 'var-$k$', 'var-$m_0$', 'var-scale', 'var-$t_0$', 'var-offset', 'var-$\\sigma$']
## Optimization Result 

* number of starts: 100 
* best value: -122181.74807659176, id=batch_8_0
* worst value: -122037.07794323802, id=batch_71_0
* number of non-finite values: 0

* execution time summary:
	* Mean execution time: 356.803s
	* Maximum execution time: 486.257s,	id=batch_4_0
	* Minimum execution time: 248.832s,	id=batch_98_0
* summary of optimizer messages:

  |   Count | Message                                         |
  |--------:|:------------------------------------------------|
  |     100 | CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH |

* best value found (approximately) 1 time(s)
* number of plateaus found: 12

A summary of the best run:

### Optimizer Result

* optimizer used: <ScipyOptimizer method=L-BFGS-B options={'disp': False, 'maxfun': 1000}>
* message: CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH 
* number of evaluations: 204
* time taken to optimize: 405.554s
* startpoint: [-3.57759052  1.32312949 -6.06454732 10.49483954 -2.9350839  -1.86882858
 -5.21399566 -1.08086353 -2.03587947 -4.76729284  3.05946839  5.38714169
  0.83592205 -3.81950452 -5.0851555 ]
* endpoint: [-0.70928777 -5.88738027  0.61305965  5.34298052  0.49761321 -0.15840209
  2.12157286 -3.48853734  0.06230154 -1.10563291  0.96063846  0.98382223
  1.58321021  2.13666286  5.99987046]
* final objective value: -122181.74807659176
* final gradient value: [-2.93095945e+00 -4.80780727e-01 -9.35076969e-01 -1.12362613e+00
 -2.52989412e+00 -1.29180262e+00 -1.00264588e+01 -8.58824933e-01
 -6.50470611e-02 -5.64381480e-01 -5.45522198e-01  1.25891529e+00
 -2.48113065e+00 -4.80271410e-01 -2.86317916e+03]

